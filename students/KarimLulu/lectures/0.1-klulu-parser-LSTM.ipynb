{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karimlulu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, Dense, Flatten, Conv1D, concatenate, Activation, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers, Model, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from conllu import parse, parse_tree\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import bz2\n",
    "import json\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport parser\n",
    "from parser import Parser\n",
    "\n",
    "%aimport helpers\n",
    "from helpers import read_embeddings, ROOT, clean_deprel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_filename = \"ubercorpus.lowercased.tokenized.300d.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.home() / \"repos/UD_Ukrainian-IU\"\n",
    "\n",
    "with list(data_dir.glob(\"*train*\"))[0].open() as f:\n",
    "    data = f.read()\n",
    "trees = parse(data)\n",
    "\n",
    "with list(data_dir.glob(\"*test*\"))[0].open() as f:\n",
    "    test_data = f.read()\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(trees, form=\"form\"):\n",
    "    word_index = {}\n",
    "    pos_index = {}\n",
    "    dep_index = {}\n",
    "    label_index = {\"shift\": 0, \"reduce\": 1}\n",
    "    for tree in trees:\n",
    "        for word in tree:\n",
    "            deprel = clean_deprel(word[\"deprel\"])\n",
    "            word_id = len(word_index)+1\n",
    "            pos_id = len(pos_index)+1\n",
    "            dep_id = len(dep_index)+1\n",
    "            label_id = len(label_index)\n",
    "            if not any(el for el in label_index.keys() if deprel in el):\n",
    "                label_index[f\"left_{deprel}\"] = label_id\n",
    "                label_index[f\"right_{deprel}\"] = label_id + 1\n",
    "            word_t = word[form].lower()\n",
    "            word_pos = word[\"upostag\"]\n",
    "            word_index[word_t] = word_index.get(word_t, word_id)\n",
    "            pos_index[word_pos] = pos_index.get(word_pos, pos_id)\n",
    "            dep_index[deprel] = dep_index.get(deprel, dep_id)\n",
    "\n",
    "    word_index[ROOT[\"form\"]] = len(word_index)+1\n",
    "    pos_index[ROOT[\"upostag\"]] = len(pos_index)+1\n",
    "    return word_index, pos_index, dep_index, label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trees, parser):\n",
    "    o_labels = []\n",
    "    o_features = []\n",
    "    for tree in trees:\n",
    "        labels, features, _ = parser.parse(tree)\n",
    "        o_labels.extend(labels)\n",
    "        o_features.extend(features)\n",
    "    return o_labels, o_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index, pos_index, dep_index, label_index = build_vocabulary(trees+test_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec, ndim, _ = read_embeddings(filename=vec_filename, word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_VEC = np.zeros(ndim, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, ndim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = word_2_vec.get(word, DEFAULT_VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1][\"id\"]], data[stack[-2][\"id\"]], data[stack[-3][\"id\"]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1][\"id\"]], data[stack[-2][\"id\"]], 0\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1][\"id\"]], 0, 0\n",
    "        else:\n",
    "            return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buffer_context(k, buffer, data):\n",
    "        if k >= 3:\n",
    "            return data[buffer[0][\"id\"]], data[buffer[1][\"id\"]], data[buffer[2][\"id\"]]\n",
    "        elif k >= 2:\n",
    "            return data[buffer[0][\"id\"]], data[buffer[1][\"id\"]], 0\n",
    "        elif k == 1:\n",
    "            return data[buffer[0][\"id\"]], 0, 0\n",
    "        else:\n",
    "            return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_context(word, deps, data, left=True):\n",
    "    if not word or word == -1:\n",
    "        return 0, (0, 0), (0, 0)\n",
    "    deps = deps[word[\"id\"]]\n",
    "    num = len(deps)\n",
    "    if not num:\n",
    "        return num, (0, 0), (0, 0)\n",
    "    elif num==1:\n",
    "        return num, (data[deps[0][0]], deps[0][1]), (0, 0)\n",
    "    else:\n",
    "        temp = sorted(deps, key=lambda x: x[0], reverse=left)\n",
    "        return num, (data[deps[0][0]], deps[0][1]), (data[deps[1][0]], deps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_builder(stack, queue, tree, parse=None, word_index=word_index, \n",
    "                    pos_index=pos_index, dep_index=dep_index, form=\"form\"):\n",
    "    words = []\n",
    "    tags = []\n",
    "    deps = []\n",
    "    depth = len(stack)\n",
    "    q_len = len(queue)\n",
    "    if ROOT not in tree:\n",
    "        tree = [ROOT, *tree]\n",
    "    \n",
    "    s0, s1, s2 = get_stack_context(depth, stack, tree)\n",
    "    q0, q1, q2 = get_buffer_context(q_len, queue, tree)\n",
    "    \n",
    "    # Left two child of the top stack\n",
    "    Ns0l, s0l1, s0l2 = get_parse_context(s0, parse.lefts, tree, left=True)  \n",
    "    # Right two child of the top stack\n",
    "    Ns0r, s0r1, s0r2 = get_parse_context(s0, parse.rights, tree, left=False)\n",
    "    # Left two child of the second element on stack\n",
    "    Ns1l, s1l1, s1l2 = get_parse_context(s1, parse.lefts, tree, left=True)\n",
    "    # Left two child of the second element on stack\n",
    "    Ns1r, s1r1, s1r2 = get_parse_context(s1, parse.rights, tree, left=False)\n",
    "    \n",
    "    if s0l1[0] and parse.lefts[s0l1[0][\"id\"]]:\n",
    "        idx, dep = max(parse.lefts[s0l1[0][\"id\"]], key=lambda x: x[0])\n",
    "        s0l1l1 = tree[idx]\n",
    "        s0l1l1_dep = dep\n",
    "    else:\n",
    "        s0l1l1 = 0\n",
    "        s0l1l1_dep = 0\n",
    "    \n",
    "    if s0r1[0] and parse.rights[s0r1[0][\"id\"]]:\n",
    "        idx, dep = min(parse.rights[s0r1[0][\"id\"]], key=lambda x: x[0])\n",
    "        s0r1r1 = tree[idx]\n",
    "        s0r1r1_dep = dep\n",
    "    else:\n",
    "        s0r1r1 = 0\n",
    "        s0r1r1_dep = 0\n",
    "    \n",
    "    if s1l1[0] and parse.lefts[s1l1[0][\"id\"]]:\n",
    "        idx, dep = max(parse.lefts[s1l1[0][\"id\"]], key=lambda x: x[0])\n",
    "        s1l1l1 = tree[idx]\n",
    "        s1l1l1_dep = dep\n",
    "    else:\n",
    "        s1l1l1 = 0\n",
    "        s1l1l1_dep = 0\n",
    "    \n",
    "    if s1r1[0] and parse.rights[s1r1[0][\"id\"]]:\n",
    "        idx, dep = min(parse.rights[s1r1[0][\"id\"]], key=lambda x: x[0])\n",
    "        s1r1r1 = tree[idx]\n",
    "        s1r1r1_dep = dep\n",
    "    else:\n",
    "        s1r1r1 = 0\n",
    "        s1r1r1_dep = 0\n",
    "#     if s0r1[0] or s0r2[0] or s0l1[0] or s0l2[0]:\n",
    "#         print(s0[\"form\"], s0[\"id\"], parse.lefts[s0[\"id\"]],  parse.rights[s0[\"id\"]])\n",
    "#         print(\"Top stack (rights): \", s0r1, s0r2)\n",
    "#         print(\"Top stack (lefts): \", s0l1, s0l2)\n",
    "#         print()\n",
    "#     if s1r1[0] or s1r2[0] or s1l1[0] or s1l2[0]:\n",
    "#         print(s1[\"form\"], s1[\"id\"], parse.lefts[s1[\"id\"]],  parse.rights[s1[\"id\"]])\n",
    "#         print(\"Second stack (rights): \", s1r1, s1r2)\n",
    "#         print(\"Second stack (lefts): \", s1l1, s1l2)\n",
    "#         print()\n",
    "    deps = [dep_index.get(s0l1[-1], 0), dep_index.get(s0l2[-1], 0),\n",
    "            dep_index.get(s0r1[-1], 0), dep_index.get(s0r2[-1], 0),\n",
    "            dep_index.get(s1l1[-1], 0), dep_index.get(s1l2[-1], 0),\n",
    "            dep_index.get(s1r1[-1], 0), dep_index.get(s1r2[-1], 0),\n",
    "            dep_index.get(s0l1l1_dep, 0), dep_index.get(s0r1r1_dep, 0),\n",
    "            dep_index.get(s1l1l1_dep, 0), dep_index.get(s1r1r1_dep, 0)]\n",
    "    \n",
    "    for x in [s0, s1, s2, q0, q1, q2, \n",
    "              s0l1[0], s0l2[0], s0r1[0], s0r2[0], \n",
    "              s1l1[0], s1l2[0], s1r1[0], s1r2[0],\n",
    "              s0l1l1, s0r1r1, s1l1l1, s1r1r1]:\n",
    "        if x:\n",
    "            word = x[form].lower() if x[\"id\"] else \"ROOT\"\n",
    "            word_idx = word_index.get(word)\n",
    "            pos_idx = pos_index.get(x[\"upostag\"])\n",
    "            words.append(word_idx)\n",
    "            tags.append(pos_idx)\n",
    "        else:\n",
    "            words.append(x)\n",
    "            tags.append(x)\n",
    "    \n",
    "    dist = s0[\"id\"] - q0[\"id\"] if q0 and s0 else 0\n",
    "    nums = [dist, Ns0l, Ns0r, Ns1r, Ns1l]\n",
    "    features = [*words, *tags, *deps, *nums]\n",
    "    return features, len(words), len(tags), len(deps), len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = trees[5]\n",
    "l, f, pairs, *_ = parser.parse(tree, feature_extractor=feature_builder)\n",
    "len(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trees, parser, feature_extractor):\n",
    "    o_labels = []\n",
    "    o_features = []\n",
    "    for tree in trees:\n",
    "        labels, features, _, n_w, n_t, n_d, n_num = parser.parse(tree, feature_extractor=feature_extractor)\n",
    "        o_labels.extend(labels)\n",
    "        o_features.extend(features)\n",
    "    return o_labels, o_features, n_w, n_t, n_d, n_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features, n_w, n_t, n_d, n_num = get_data(trees+test_trees, parser, feature_builder)\n",
    "label_index = parser.label_index.copy()\n",
    "X = np.asarray(features)\n",
    "y = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 154709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_layer = Embedding(len(word_index)+1,\n",
    "                            ndim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=n_w,\n",
    "                            trainable=0\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer = Embedding(len(pos_index)+1,\n",
    "                                50,\n",
    "                                input_length=n_t,\n",
    "                                trainable=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_embedding_layer = Embedding(len(dep_index)+1,\n",
    "                                50,\n",
    "                                input_length=n_d,\n",
    "                                trainable=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence_input = Input(shape=(n_w,), dtype='int32')\n",
    "word_embedded_sequences = word_embedding_layer(word_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sequence_input = Input(shape=(n_t,), dtype='int32')\n",
    "pos_embedded_sequences = pos_embedding_layer(pos_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_sequence_input = Input(shape=(n_d,), dtype='int32')\n",
    "dep_embedded_sequences = dep_embedding_layer(dep_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Input(shape=(n_num,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = Flatten()(word_embedded_sequences)\n",
    "pos = Flatten()(pos_embedded_sequences)\n",
    "dep = Flatten()(dep_embedded_sequences)\n",
    "x = concatenate(inputs=[word, pos, dep, features], axis=-1)\n",
    "#x = concatenate(inputs=[word_embedded_sequences, pos_embedded_sequences], axis=1)\n",
    "#x = LSTM(256)(x)\n",
    "x = Dense(200, activation=\"relu\",\n",
    "          kernel_regularizer=regularizers.l2(1e-8))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "#x = Dense(200, activation=\"relu\",\n",
    "#          kernel_regularizer=regularizers.l2(1e-8))(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "preds = Dense(len(parser.label_index), activation='softmax',\n",
    "             kernel_regularizer=regularizers.l2(1e-8))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[word_sequence_input, pos_sequence_input, dep_sequence_input, features], outputs=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154709 samples, validate on 30661 samples\n",
      "Epoch 1/6\n",
      "154709/154709 [==============================] - 61s 396us/step - loss: 0.6020 - acc: 0.8225 - val_loss: 0.4323 - val_acc: 0.8616\n",
      "Epoch 2/6\n",
      "154709/154709 [==============================] - 61s 395us/step - loss: 0.3706 - acc: 0.8827 - val_loss: 0.4004 - val_acc: 0.8687\n",
      "Epoch 3/6\n",
      "154709/154709 [==============================] - 64s 414us/step - loss: 0.3136 - acc: 0.9000 - val_loss: 0.3839 - val_acc: 0.8725\n",
      "Epoch 4/6\n",
      "154709/154709 [==============================] - 67s 430us/step - loss: 0.2789 - acc: 0.9109 - val_loss: 0.3756 - val_acc: 0.8741\n",
      "Epoch 5/6\n",
      "154709/154709 [==============================] - 68s 441us/step - loss: 0.2529 - acc: 0.9199 - val_loss: 0.3724 - val_acc: 0.8757\n",
      "Epoch 6/6\n",
      "154709/154709 [==============================] - 69s 448us/step - loss: 0.2321 - acc: 0.9264 - val_loss: 0.3680 - val_acc: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ad7b73198>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [X_train[:, :n_w], X_train[:, n_w:n_w+n_t], X_train[:, n_w+n_t:n_w+n_t+n_d], X_train[:, n_w+n_t+n_d:]], \n",
    "          y_train, \n",
    "    validation_data=([X_test[:, :n_w], X_test[:, n_w:n_w+n_t], X_test[:, n_w+n_t:n_w+n_t+n_d], X_test[:, n_w+n_t+n_d:]], y_test), \n",
    "          epochs=6, \n",
    "          batch_size=128, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LUAS(parser, trees, oracle=None, feature_extractor=None):\n",
    "    total, tpL, tpU, failed = 0, 0, 0, 0\n",
    "    for tree in trees:\n",
    "        golden = [(node[\"id\"], node[\"head\"], clean_deprel(node[\"deprel\"])) for node in tree]\n",
    "        try:\n",
    "            _, _, predicted, *_ = parser.parse(tree, oracle=oracle, update_label_index=False,\n",
    "                                               feature_extractor=feature_extractor)\n",
    "            total += len(golden)\n",
    "            tpL += len(set(golden).intersection(set(predicted)))\n",
    "            tpU += len(set([(c,h) for c,h,_ in golden]).intersection([(c,h) for c,h,_ in predicted]))\n",
    "        except:\n",
    "            failed += 1\n",
    "    return total, tpL, tpU, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: 1\n",
      "Total: 14869\n",
      "Correctly defined (unlabeled): 11136\n",
      "Correctly defined (labeled): 10398\n",
      "UAS: 0.749\n",
      "LAS: 0.699\n"
     ]
    }
   ],
   "source": [
    "total, tpL, tpU, failed = LUAS(parser, test_trees, model, feature_extractor=feature_builder)\n",
    "print(\"Failed:\", failed)\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined (unlabeled):\", tpU)\n",
    "print(\"Correctly defined (labeled):\", tpL)\n",
    "print(\"UAS:\", round(tpU / total, 3))\n",
    "print(\"LAS:\", round(tpL / total, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
