{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, recall_score, \n",
    "                             confusion_matrix, accuracy_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop = set(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, pred, proba=None, labels=[\"ham\", \"spam\"], print_=True):\n",
    "    output = {}\n",
    "    if proba is not None:\n",
    "        roc_auc = roc_auc_score(y_test, proba)\n",
    "        output[\"AUC\"] = roc_auc\n",
    "    output[\"Recall\"] = recall_score(y_test, pred)\n",
    "    output[\"Precision\"] = precision_score(y_test, pred)\n",
    "    output[\"F1\"] = f1_score(y_test, pred)\n",
    "    output[\"accuracy\"] = accuracy_score(y_test, pred)\n",
    "    if labels is not None:\n",
    "        index = labels\n",
    "        columns = [\"pred_\" + el for el in index]\n",
    "    else:\n",
    "        columns = None\n",
    "        index = None\n",
    "    output[\"conf_matrix\"] = pd.DataFrame(confusion_matrix(y_test, pred), \n",
    "                                         columns=columns, index=index)\n",
    "    if print_:\n",
    "        for key, value in output.items():\n",
    "            if \"matrix\" in key:\n",
    "                print(value)\n",
    "            else:\n",
    "                print(f\"{key}: {value:0.3f}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocess input document\n",
    "    \"\"\"\n",
    "    def __init__(self, stopwords=stop, remove_stop=True, min_length=0):\n",
    "        self.stopwords = stopwords\n",
    "        self.remove_stop = remove_stop\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        if not isinstance(doc, (str, bytes)):\n",
    "            doc = str(doc)\n",
    "        if self.remove_stop:\n",
    "            tokens = [el.strip(' \\n\\t\\r').lower() for el in doc.split() if el.strip().lower() not in self.stopwords]\n",
    "        else:\n",
    "            tokens = [el.strip(' \\n\\t\\r').lower() for el in doc.split()]\n",
    "        return ' '.join(filter(lambda x: len(x)>=self.min_length, tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"SMSSpamCollection.txt\"\n",
    "df = pd.read_csv(Path() / \"data\" / filename, sep=\"\\t\", header=None, names=[\"label\", \"sms\"])\n",
    "mapping = {\"ham\": 0,\n",
    "           \"spam\": 1}\n",
    "df[\"label\"] = df[\"label\"].map(mapping)\n",
    "df[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 4457, Num. of test: 1115\n"
     ]
    }
   ],
   "source": [
    "X = df[\"sms\"]\n",
    "y = df[\"label\"]\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's implement NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_likelihood(data, vectorizer, y):\n",
    "    tfidf =  vectorizer.fit_transform(data).toarray()\n",
    "    prob_spam = tfidf[y > 0].sum(axis=0) / np.sum(tfidf)\n",
    "    prob_ham = tfidf[y < 1].sum(axis=0) / np.sum(tfidf)\n",
    "    return np.vstack((prob_ham, prob_spam)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, vectorizer, priors, likelihood):\n",
    "    tf = vectorizer.transform(X_test).toarray() \n",
    "    prob = tf[:, :, np.newaxis] * likelihood[np.newaxis, :, :]\n",
    "    prob[prob==0] = 1\n",
    "    score = np.exp(np.log(prob).sum(axis=1)) * priors\n",
    "    score[score.max(axis=1)==0] = eps\n",
    "    return score / score.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params = {\"lowercase\": True,\n",
    "             \"analyzer\": \"char_wb\",\n",
    "             \"stop_words\": stop,\n",
    "             \"ngram_range\": (3, 3),\n",
    "             \"min_df\": 1,\n",
    "             \"max_df\": 1.0,\n",
    "             \"preprocessor\": None,#Preprocessor(),\n",
    "             \"max_features\": 3500,\n",
    "             \"norm\": None,\n",
    "             \"use_idf\": True\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-15\n",
    "priors = y_train.value_counts(normalize=True).values\n",
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.940\n",
      "Recall: 0.839\n",
      "Precision: 0.806\n",
      "F1: 0.822\n",
      "accuracy: 0.952\n",
      "      pred_ham  pred_spam\n",
      "ham        936         30\n",
      "spam        24        125\n"
     ]
    }
   ],
   "source": [
    "likelihood = calc_likelihood(X_train, vectorizer, y_train)\n",
    "likelihood[likelihood==0] = eps\n",
    "likelihood[likelihood==1] = 1 - eps\n",
    "assert np.amax(np.fabs(np.sum(likelihood, axis=1) - train.toarray().sum(axis=0) / np.sum(train))) <= 1e-5\n",
    "pred_probs = predict(X_test, vectorizer, priors, likelihood)\n",
    "proba = pred_probs[:, 1]\n",
    "pred = np.zeros_like(proba)\n",
    "pred[proba>=0.5] = 1\n",
    "metrics = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Scikit learn NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.984\n",
      "Recall: 0.953\n",
      "Precision: 0.928\n",
      "F1: 0.940\n",
      "accuracy: 0.984\n",
      "      pred_ham  pred_spam\n",
      "ham        955         11\n",
      "spam         7        142\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=2.5, class_prior=[0.5, 0.5])\n",
    "clf.fit(train.toarray(), y_train)\n",
    "pred = clf.predict(test.toarray())\n",
    "proba = clf.predict_proba(test.toarray())[:, 1]\n",
    "metrics = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['scale_pos_weight'] = sum(y_train==0) / sum(y_train==1)\n",
    "params['learning_rate'] = 0.1\n",
    "params['n_estimators'] = 1000\n",
    "params['max_depth'] = 5\n",
    "params['min_child_weight'] = 100\n",
    "params['gamma'] = 0\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['seed'] = 27\n",
    "params['n_jobs'] = -1\n",
    "params[\"eval_metric\"] = [\"error\",\"auc\"]\n",
    "params[\"early_stopping_rounds\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, y_train)\n",
    "dtest = xgb.DMatrix(test, y_test)\n",
    "eval_set = [(dtrain, \"train\"), (dtest, \"eval\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.189141\ttrain-auc:0.918843\teval-error:0.202691\teval-auc:0.904508\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "[50]\ttrain-error:0.056316\ttrain-auc:0.982994\teval-error:0.063677\teval-auc:0.973439\n",
      "[100]\ttrain-error:0.049585\ttrain-auc:0.984981\teval-error:0.061883\teval-auc:0.972814\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-error:0.05295\ttrain-auc:0.983661\teval-error:0.061883\teval-auc:0.974009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(dtrain=dtrain, num_boost_round=params.get(\"n_estimators\"), \n",
    "                  early_stopping_rounds=params.get(\"early_stopping_rounds\"), \n",
    "                  params=params, evals=eval_set, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.973\n",
      "Recall: 0.886\n",
      "Precision: 0.725\n",
      "F1: 0.798\n",
      "accuracy: 0.940\n",
      "      pred_ham  pred_spam\n",
      "ham        916         50\n",
      "spam        17        132\n"
     ]
    }
   ],
   "source": [
    "pred_proba_xgb = model.predict(dtest)\n",
    "pred_xgb = np.zeros_like(pred_proba_xgb)\n",
    "pred_xgb[pred_proba_xgb>=0.5] = 1\n",
    "xgb_metrics = calc_metrics(y_test, pred_xgb, pred_proba_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
